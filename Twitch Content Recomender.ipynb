{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install necessary libaries\n",
        "!pip install numpy scipy dask distributed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5epmKDD7WAZ",
        "outputId": "c02752ad-23d6-4721-e973-ce48507eef08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.10.3->distributed) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrPlowGv5sqY",
        "outputId": "40842b95-d283-4f17-8197-2d1fb9929bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:41343\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44163'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:39589 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:39589\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:33168\n",
            "INFO:distributed.scheduler:Receive client connection: Client-4cf5ee48-3f61-11f0-8596-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56814\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45491'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42685'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45863'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46793'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36601'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:32769'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44819'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46293 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46293\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56910\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46399 name: 7\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46399\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56932\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:45949 name: 6\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:45949\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56916\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:45989 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:45989\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56920\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46499 name: 5\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46499\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56938\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:44065 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:44065\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56926\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:32875 name: 4\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:32875\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "     Predictions\n",
            "     (Streamer Name , prediction ranking value) \n",
            "========================================\n",
            "\n",
            "kendinemuzisyen 0.38217882779290085\n",
            "jahrein 0.0903926596543872\n",
            "zeon 0.0803713280777501\n",
            "wtcn 0.05443016584179818\n",
            "zeusidiouss 0.05238674605885096\n",
            "jrokezftw 0.046176582873830585\n",
            "jtgtv 0.037216710825417776\n",
            "mithrain 0.027529210755798957\n",
            "ogrencievi 0.022496141632371514\n",
            "elraenn 0.022386812243139056\n",
            "raufbaba25 0.019306643744068014\n",
            "unlostv 0.015498847223790968\n",
            "towshun 0.015498847223790968\n",
            "grimnax 0.015498847223790968\n",
            "pqueen 0.01287509659069292\n",
            "elwind 0.012147468569486918\n",
            "teasy 0.009906982670744139\n",
            "alptv 0.008113237622498486\n",
            "berkriptepe 0.007385609601292482\n",
            "pintipanda 0.006548652190854026\n",
            "esl_csgo 0.005821024169648023\n",
            "tecone 0.00509339614844202\n",
            "zade 0.00509339614844202\n",
            "beatpug 0.00509339614844202\n",
            "xantarescn 0.00509339614844202\n",
            "imorr 0.004365768127236017\n",
            "h3x_tv 0.002461869867097494\n",
            "videoyun 0.0021828840636180084\n",
            "educatedeartw 0.0021828840636180084\n",
            "glaxycsgo 0.0021828840636180084\n",
            "lurzbob 0.0014552560424120058\n",
            "s1mple 0.0014552560424120058\n",
            "mertgunhan 0.0014552560424120058\n",
            "canku77 0.0014552560424120058\n",
            "agnosta 0.0014552560424120058\n",
            "gandhitw 0.0014552560424120058\n",
            "cansungur 0.0007276280212060029\n",
            "targetlocated 0.0007276280212060029\n",
            "zelenta 0.0007276280212060029\n",
            "thisisczentovic 0.0007276280212060029\n",
            "feitv 0.0007276280212060029\n",
            "raedmoss 0.0007276280212060029\n",
            "volentes 0.0007276280212060029\n",
            "d0cc_tv 0.0007276280212060029\n",
            "mrcarbonate 0.0007276280212060029\n",
            "thuraten 0.0007276280212060029\n",
            "turgutuc 0.0007276280212060029\n",
            "civcivv 0.0007276280212060029\n",
            "keycode 0.0007276280212060029\n",
            "pembepati 0.0007276280212060029\n",
            "sinemasyonnn 0.0007276280212060029\n",
            "purplebixi 0.0007276280212060029\n",
            "benmestefe 0.0007276280212060029\n",
            "legatusleman 0.0007276280212060029\n",
            "\n",
            "========================================\n",
            "     Input Data (ie Data Seen By Model)\n",
            "     (Streamer Name , total watch time for input user)\n",
            "========================================\n",
            "\n",
            "kendinemuzisyen 32\n",
            "wtcn 11\n",
            "mithrain 10\n",
            "jahrein 9\n",
            "grimnax 6\n",
            "towshun 6\n",
            "unlostv 6\n",
            "jtgtv 5\n",
            "ogrencievi 5\n",
            "raufbaba25 4\n",
            "zeusidiouss 3\n",
            "berkriptepe 3\n",
            "alptv 3\n",
            "elraenn 2\n",
            "h3x_tv 1\n",
            "\n",
            "========================================\n",
            "     Recomendations\n",
            "========================================\n",
            "\n",
            "zeon 0.0803713280777501\n",
            "jrokezftw 0.046176582873830585\n",
            "pqueen 0.01287509659069292\n",
            "elwind 0.012147468569486918\n",
            "teasy 0.009906982670744139\n",
            "pintipanda 0.006548652190854026\n",
            "esl_csgo 0.005821024169648023\n",
            "tecone 0.00509339614844202\n",
            "zade 0.00509339614844202\n",
            "beatpug 0.00509339614844202\n",
            "xantarescn 0.00509339614844202\n",
            "imorr 0.004365768127236017\n",
            "videoyun 0.0021828840636180084\n",
            "educatedeartw 0.0021828840636180084\n",
            "glaxycsgo 0.0021828840636180084\n",
            "lurzbob 0.0014552560424120058\n",
            "s1mple 0.0014552560424120058\n",
            "mertgunhan 0.0014552560424120058\n",
            "canku77 0.0014552560424120058\n",
            "agnosta 0.0014552560424120058\n",
            "\n",
            "========================================\n",
            "     Omitted Data\n",
            "========================================\n",
            "\n",
            "['jrokezftw', 'zeon', 'eraymaskulen', 'esl_csgo']\n",
            "\n",
            "====================\n",
            "     Correct Recomendations\n",
            "====================\n",
            "\n",
            "zeon\n",
            "jrokezftw\n",
            "esl_csgo\n",
            "\n",
            "Recall at K :  0.75\n"
          ]
        }
      ],
      "source": [
        "####################################################################################################\n",
        "## NOTE : the comments in this code were generated with the help of AI , the code itself was not. ##\n",
        "####################################################################################################\n",
        "\n",
        "# import neccesary libraries\n",
        "\n",
        "import dask.bag as db\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from dask.distributed import Client, LocalCluster\n",
        "import urllib.request\n",
        "import time\n",
        "import sys\n",
        "import random\n",
        "\n",
        "def run_model(UserIDofInterest , problemSize , numberOfWorkers , k):\n",
        "    \"\"\"\n",
        "    Runs a collaborative filtering recommendation model to suggest streamers to a user.\n",
        "\n",
        "    Args:\n",
        "        UserIDofInterest (int): The ID of the user for whom to generate recommendations.\n",
        "        problemSize (int): The number of records to process from the dataset.\n",
        "        numberOfWorkers (int): The number of Dask workers to use for parallel computation.\n",
        "        k (int): The number of top recommendations to display.\n",
        "\n",
        "        please insure that the UserIDofInterest < problemSize / 30 , this will guarantee that the target user is in the problem set\n",
        "    \"\"\"\n",
        "\n",
        "    # download data set (100k csv file)\n",
        "    filename = '100k_a.csv'\n",
        "    urllib.request.urlretrieve('https://mcauleylab.ucsd.edu/public_datasets/gdrive/twitch/100k_a.csv', filename) # Downloads the 100k_a.csv dataset from mcauleylab.ucsd.edu.\n",
        "\n",
        "\n",
        "    cluster.scale(numberOfWorkers) # Scales the Dask cluster to the specified number of workers.\n",
        "    client.wait_for_workers(numberOfWorkers) # Waits until all specified workers are ready.\n",
        "\n",
        "\n",
        "    # load data into dask bag and preprocess\n",
        "    def processIntoLists(line):\n",
        "        \"\"\"\n",
        "        Processes a single line of input data into a list of strings.\n",
        "        Removes newline characters and splits the line by commas.\n",
        "        \"\"\"\n",
        "        line = line.rstrip('\\n') # Removes trailing newline character.\n",
        "        line = list(line.split(',')) # Splits the string by commas and converts it to a list.\n",
        "        return line\n",
        "\n",
        "\n",
        "    def doWatchTimeprocessing(line):\n",
        "        \"\"\"\n",
        "        Calculates the watch time for a record by subtracting the start time from the end time.\n",
        "        \"\"\"\n",
        "        line[-1] = int(line[-1]) - int(line[-2]) # Calculates watch time (end_time - start_time).\n",
        "        return line\n",
        "\n",
        "\n",
        "    def discardStreamID(line):\n",
        "        \"\"\"\n",
        "        Discards specific elements (Stream IDs) from the input line.\n",
        "        Assumes elements at index 1 and 2 are to be removed.\n",
        "        \"\"\"\n",
        "        del line[1] # Deletes the element at index 1 (Stream ID).\n",
        "        del line[2]\n",
        "        return line\n",
        "\n",
        "\n",
        "    def preProcessing(inputBag):\n",
        "        \"\"\"\n",
        "        Applies a series of preprocessing steps to a Dask Bag.\n",
        "        Includes converting lines to lists, calculating watch times, and discarding stream IDs.\n",
        "        \"\"\"\n",
        "        inputBag = inputBag.map(processIntoLists) # Applies processIntoLists to each element in the bag.\n",
        "        inputBag = inputBag.map(doWatchTimeprocessing) # Applies doWatchTimeprocessing to each element.\n",
        "        inputBag = inputBag.map(discardStreamID) # Applies discardStreamID to each element.\n",
        "        return inputBag\n",
        "\n",
        "\n",
        "    bag = db.read_text('100k_a.csv') # Reads the CSV file into a Dask Bag.\n",
        "    bag = preProcessing(bag) # Applies the defined preprocessing steps to the Dask Bag.\n",
        "    bag = bag.persist() # Persists the Dask Bag in memory, forcing computation and making it available for subsequent operations.\n",
        "    bag = bag.take(problemSize , compute = False) # Takes a problemSize number of elements from the bag without immediate computation.\n",
        "\n",
        "\n",
        "    def createLookupTable(inputBag):\n",
        "        \"\"\"\n",
        "        Creates a dictionary mapping streamer names to unique integer IDs.\n",
        "        Extracts distinct streamer names from the input bag and assigns incremental IDs.\n",
        "        \"\"\"\n",
        "        distinctBag = inputBag.distinct(key = lambda x: x[1]) # Gets distinct streamer names (at index 1) from the bag.\n",
        "        streamerNames = distinctBag.map(lambda x: x[1]) # Extracts just the streamer names.\n",
        "        streamerNamesList = list(streamerNames) # Collects streamer names into a local list.\n",
        "\n",
        "        streamerIDlookUpDict = dict() # Initializes an empty dictionary for the lookup table.\n",
        "        for i, steamerName in enumerate(streamerNamesList): # Iterates through distinct streamer names.\n",
        "            streamerIDlookUpDict[steamerName] = i # Assigns a unique integer ID to each streamer name.\n",
        "        return streamerIDlookUpDict\n",
        "\n",
        "    streamerIDlookUpDict = createLookupTable(bag) # Creates the streamer name to ID lookup dictionary.\n",
        "\n",
        "\n",
        "    def mapStreamerNametoStreamerID(line):\n",
        "        \"\"\"\n",
        "        Maps the streamer name in a record to its corresponding integer ID using the lookup dictionary.\n",
        "        \"\"\"\n",
        "        line[1] = streamerIDlookUpDict[line[1]] # Replaces the streamer name with its ID.\n",
        "        return line\n",
        "\n",
        "    transformedBag = bag.map(mapStreamerNametoStreamerID) # Applies the mapping function to the Dask Bag.\n",
        "\n",
        "\n",
        "    def combinedKey(x):\n",
        "        \"\"\"\n",
        "        Creates a combined key from UserID and Streamer ID for grouping.\n",
        "        \"\"\"\n",
        "        return x[0], x[1] # Returns a tuple of UserID and StreamerID.\n",
        "\n",
        "    # Folds the bag, grouping by (UserID, StreamerID) and summing the watch times (x[2]) for each unique pair.\n",
        "    folded = transformedBag.foldby(key = combinedKey , binop = lambda acc,x: acc + x[2] , initial=0 , combine = lambda x,y: x+y)\n",
        "\n",
        "\n",
        "    def reformatData(x):\n",
        "        \"\"\"\n",
        "        Reformats the data from the folded bag into a (UserID, StreamerID, WatchTime) tuple.\n",
        "        Ensures UserID is an integer.\n",
        "        \"\"\"\n",
        "        userID , StreamerID = x[0] # Unpacks the combined key.\n",
        "        watchTime = x[1] # Gets the summed watch time.\n",
        "        return (int(userID) , StreamerID , watchTime) # Returns the reformatted tuple.\n",
        "\n",
        "    folded2 = folded.map(reformatData) # Applies the reformatting function to the folded bag.\n",
        "\n",
        "    # group by userID\n",
        "    foldedGrouped = folded2.groupby(lambda x: x[0]).persist() # Groups the data by UserID and persists the result.\n",
        "\n",
        "\n",
        "    # Filters the grouped data to get the specific UserID of interest and extracts their streamer watch data.\n",
        "    fullDataSet = foldedGrouped.filter(lambda x : x[0] == UserIDofInterest).pluck(1)\n",
        "    fullDataSet = fullDataSet.compute()[0] # Computes and retrieves the full watch data for the user of interest.\n",
        "\n",
        "\n",
        "    def predicitonTestingFunc(line , UserIDofInterest):\n",
        "        \"\"\"\n",
        "        Omits specific data points (streamer watch entries) for the target user to simulate unknown preferences for testing.\n",
        "        This allows for evaluating the model's ability to predict these \"omitted\" values.\n",
        "        \"\"\"\n",
        "        userID , data = line[0], line[1] # Unpacks UserID and their associated data.\n",
        "        if userID == UserIDofInterest: # Checks if the current line belongs to the target user.\n",
        "            data[3:4] = [] # Omits data at index 3.\n",
        "            data[6:7] = [] # Omits data at index 6.\n",
        "            data[10:11] = [] # Omits data at index 10.\n",
        "            data[13:14] = [] # Omits data at index 13.\n",
        "        return (userID , data) # Returns the modified user data.\n",
        "\n",
        "    # Applies the prediction testing function to the grouped data, omitting some entries for the target user.\n",
        "    foldedGroupedOmitted = foldedGrouped.map(predicitonTestingFunc , UserIDofInterest).persist()\n",
        "\n",
        "\n",
        "    def mappingFunct2(line):\n",
        "        \"\"\"\n",
        "        Transforms a user's watch data into a NumPy array (vector) where each index\n",
        "        corresponds to a StreamerID and the value at that index is the watch time.\n",
        "        Initializes a large array and populates it based on the streamer IDs and watch times.\n",
        "        \"\"\"\n",
        "        userID , data = line[0] , line[1] # Unpacks UserID and their watch data.\n",
        "        v = np.zeros(170000) # Initializes a NumPy array of zeros with a size large enough to cover all possible streamer IDs.\n",
        "\n",
        "        for element in data: # Iterates through each streamer watch entry for the user.\n",
        "            v[element[1]] = element[2] # Populates the array: index is StreamerID, value is watch time.\n",
        "\n",
        "        return (userID , v) # Returns the UserID and their watch preference vector.\n",
        "\n",
        "    data = foldedGroupedOmitted.map(mappingFunct2) # Applies the mapping function to create watch preference vectors for all users.\n",
        "\n",
        "\n",
        "    def datanormalisationfunct(line):\n",
        "        \"\"\"\n",
        "        Normalizes a user's watch preference array so that watch time values are percentages\n",
        "        of that user's total watch time. This creates a probability distribution of watch preferences.\n",
        "        \"\"\"\n",
        "        userID , array = line[0] , line[1] # Unpacks UserID and their watch preference array.\n",
        "        array = array / array.sum() # Divides each element by the sum of all elements to get percentages.\n",
        "\n",
        "        return (userID , array) # Returns the UserID and their normalized watch preference array.\n",
        "\n",
        "    datanormalisation = data.map(datanormalisationfunct) # Applies the normalization function to all user watch arrays.\n",
        "\n",
        "\n",
        "    def downCastArrayDtype(line):\n",
        "        \"\"\"\n",
        "        Downcasts the data type of the NumPy arrays to float16 to reduce memory usage.\n",
        "        \"\"\"\n",
        "        userID , array = line[0] ,line[1] # Unpacks UserID and array.\n",
        "        return userID , array.astype(np.float16) # Returns UserID and the array cast to float16.\n",
        "\n",
        "    datanormalisation = datanormalisation.map(downCastArrayDtype) # Applies the downcasting function.\n",
        "\n",
        "\n",
        "    datanormalisation = datanormalisation.persist() # Persists the normalized and downcasted data in memory.\n",
        "\n",
        "\n",
        "    def getUserInputArray(UserIDofInterest):\n",
        "        \"\"\"\n",
        "        Retrieves the normalized watch preference array for the specific target user.\n",
        "        \"\"\"\n",
        "        # inputUserArray = list(datanormalisation.filter(lambda x: x[0] == UserIDofInterest))[0][1].flatten()\n",
        "\n",
        "        # Filters the normalized data to find the target user's array, takes the first result,\n",
        "        # extracts the array, and flattens it to a 1D array.\n",
        "        inputUserArray = datanormalisation.filter(lambda x: x[0] == UserIDofInterest).take(1)[0][1].flatten()\n",
        "\n",
        "        return inputUserArray # Returns the target user's watch preference array.\n",
        "\n",
        "\n",
        "    def cosineSimilaritys2(givenUser):\n",
        "        \"\"\"\n",
        "        Calculates the cosine similarity between the target user's watch preference array\n",
        "        and another given user's watch preference array.\n",
        "        \"\"\"\n",
        "        userID, givenUserMatrix = givenUser[0] , givenUser[1] # Unpacks the given user's ID and matrix.\n",
        "        givenUserArray = np.array(givenUserMatrix).flatten() # Flattens the given user's matrix into a 1D array.\n",
        "        cosineSimilarity = 1 - cosine(inputUserArray , givenUserArray) # Calculates cosine similarity (1 - cosine distance).\n",
        "        return (userID , cosineSimilarity) # Returns the given user's ID and their cosine similarity to the target user.\n",
        "\n",
        "    def computeCosineSimilarities( inputUserArray , datanormalisation ):\n",
        "        \"\"\"\n",
        "        Computes cosine similarities between the target user and all other users in the dataset.\n",
        "        \"\"\"\n",
        "        cosineSimilaritys = datanormalisation.map(cosineSimilaritys2) # Applies the cosine similarity calculation to all users.\n",
        "        return cosineSimilaritys\n",
        "\n",
        "    inputUserArray = getUserInputArray(UserIDofInterest) # Gets the target user's normalized watch preference array.\n",
        "\n",
        "    cosineSimilairtys = computeCosineSimilarities( inputUserArray , datanormalisation ) # Computes cosine similarities for all users.\n",
        "\n",
        "\n",
        "    # Selects the top 5 users with the highest cosine similarity to the target user.\n",
        "    topknearestneighbours = cosineSimilairtys.topk(5 , key = lambda x: x[1]).persist()\n",
        "\n",
        "    cosineSimilarityValues = topknearestneighbours.pluck(1) # Extracts only the cosine similarity values from the nearest neighbors.\n",
        "\n",
        "    totalCosineSimilarity = cosineSimilarityValues.sum() # Calculates the sum of these cosine similarity values.\n",
        "\n",
        "\n",
        "    def upsize(input):\n",
        "        \"\"\"\n",
        "        Upsizes the data type of the arrays back to float64 for increased precision in subsequent calculations.\n",
        "        \"\"\"\n",
        "        userID , array = input[0] , input[1] # Unpacks UserID and array.\n",
        "        array = array.astype(np.float64) # Casts the array to float64.\n",
        "        return userID , array # Returns UserID and the upsized array.\n",
        "\n",
        "    topknearestneighbours = topknearestneighbours.map(upsize) # Applies the upsize function to the nearest neighbors' data.\n",
        "\n",
        "\n",
        "    def normaliseCosineSimilarityValues(line , totalCosineSimilarity):\n",
        "        \"\"\"\n",
        "        Normalizes the cosine similarity values of the nearest neighbors.\n",
        "        Each similarity value is divided by the total sum of all similarities of the nearest neighbors.\n",
        "        \"\"\"\n",
        "        userID , value = line[0] , line[1] # Unpacks UserID and cosine similarity value.\n",
        "        value = value /totalCosineSimilarity # Normalizes the value.\n",
        "        return (userID , value) # Returns UserID and the normalized cosine similarity.\n",
        "\n",
        "    # Applies the normalization to the nearest neighbors' cosine similarity values and persists the result.\n",
        "    topknearestneighboursNormlaised = topknearestneighbours.map(normaliseCosineSimilarityValues , totalCosineSimilarity).persist()\n",
        "\n",
        "    nearestNeighbourIDs = topknearestneighboursNormlaised.pluck(0) # Extracts only the UserIDs of the normalized nearest neighbors.\n",
        "\n",
        "    # Joins the normalized full dataset with the nearest neighbor IDs to filter for only the nearest neighbors' data.\n",
        "    nearestneighboursDataNormilisation = datanormalisation.join(nearestNeighbourIDs , on_self = lambda x : x[0] , on_other = lambda x : x)\n",
        "\n",
        "    def reformat(line):\n",
        "        \"\"\"\n",
        "        Reformats the joined data, extracting only the relevant data from the joined tuple.\n",
        "        \"\"\"\n",
        "        extraUserID , dataWeWant = line[0] , line[1] # Unpacks the joined tuple.\n",
        "        return dataWeWant # Returns only the desired data.\n",
        "\n",
        "    nearestneighboursDataNormilisation = nearestneighboursDataNormilisation.map(reformat) # Applies the reformatting.\n",
        "\n",
        "\n",
        "    def normaliseWatchTimes(line):\n",
        "        \"\"\"\n",
        "        Normalizes the watch times within each nearest neighbor's data.\n",
        "        Each streamer's watch time for a given neighbor is expressed as a percentage of that neighbor's total watch time.\n",
        "        \"\"\"\n",
        "        UserID , data = line # Unpacks UserID and their associated watch data.\n",
        "        totalWatchTime = 0\n",
        "        for element in data: # Calculates the total watch time for the current user.\n",
        "            totalWatchTime += element[2]\n",
        "        normalisedData = []\n",
        "        for element in data: # Normalizes each streamer's watch time.\n",
        "            normalisedElement = (element[0] , element[1] , element[2] / totalWatchTime) # Creates a new tuple with normalized watch time.\n",
        "            normalisedData.append(normalisedElement)\n",
        "        return UserID , normalisedData # Returns UserID and their normalized watch data.\n",
        "\n",
        "    normalisedfoldedGroupedOmitted = foldedGroupedOmitted.map(normaliseWatchTimes) # Applies watch time normalization to the omitted grouped data.\n",
        "\n",
        "    # Joins the normalized omitted data with the normalized nearest neighbors (by cosine similarity)\n",
        "    # to combine their watch data with their normalized similarity scores.\n",
        "    onlyNearestNeighbours = normalisedfoldedGroupedOmitted.join(topknearestneighboursNormlaised , lambda x : x[0])\n",
        "\n",
        "\n",
        "    def weightByCosineSimilarity(line):\n",
        "        \"\"\"\n",
        "        Weights each streamer's watch time by the corresponding nearest neighbor's normalized cosine similarity.\n",
        "        This gives more importance to recommendations from more similar neighbors.\n",
        "        \"\"\"\n",
        "        userID , cosineSimilarity = line[0] # Unpacks UserID and normalized cosine similarity.\n",
        "        UserID2 , data = line[1] # Unpacks the neighbor's UserID and their normalized watch data.\n",
        "\n",
        "        weighted_data = []\n",
        "        for element in data: # Iterates through each streamer in the neighbor's watch data.\n",
        "            element = list(element) # Converts the tuple to a list to modify.\n",
        "            element[2] = element[2] * cosineSimilarity # Weights the watch time by cosine similarity.\n",
        "            weighted_data.append(tuple(element)) # Converts back to tuple and appends.\n",
        "\n",
        "        return (userID, weighted_data) # Returns the neighbor's UserID and their weighted watch data.\n",
        "\n",
        "    weightedOnlyNearestNeighbours = onlyNearestNeighbours.map(weightByCosineSimilarity) # Applies the weighting function.\n",
        "\n",
        "\n",
        "    def reformating(line):\n",
        "        \"\"\"\n",
        "        Reformats the weighted data by removing the UserID from each streamer's watch entry.\n",
        "        \"\"\"\n",
        "        UserID = line[0] # Unpacks UserID.\n",
        "        data = line[1] # Unpacks the weighted watch data.\n",
        "        for index , element in enumerate(data): # Iterates through each streamer entry.\n",
        "            element = list(element) # Converts to list.\n",
        "            del element[0] # Deletes the UserID from the streamer entry.\n",
        "            data[index] = tuple(element) # Converts back to tuple.\n",
        "\n",
        "        return data # Returns the reformatted (StreamerID, weighted_watch_time) list.\n",
        "\n",
        "    # Applies the reformatting and flattens the bag, resulting in a bag of (StreamerID, weighted_watch_time) tuples.\n",
        "    refrommatedWeightedOnlyNearestNeighbours = weightedOnlyNearestNeighbours.map(reformating).flatten()\n",
        "\n",
        "\n",
        "\n",
        "    # Folds the reformatted data, grouping by StreamerID and summing the weighted watch times.\n",
        "    # The sum represents the prediction score for each streamer.\n",
        "    groupedStreamerWatchTime = refrommatedWeightedOnlyNearestNeighbours.foldby(key = lambda x : x[0] , binop = lambda acc , x : acc + x[1] , initial = 0)\n",
        "\n",
        "    # Repartitions the bag for better parallelism and persists the result.\n",
        "    groupedStreamerWatchTime = groupedStreamerWatchTime.repartition(numberOfWorkers * 2).persist()\n",
        "\n",
        "\n",
        "    # Filters out predictions below a certain threshold to keep only significant recommendations.\n",
        "    groupedStreamerWatchTime = groupedStreamerWatchTime.filter(lambda x : x[1] > 0.0005 ).persist()\n",
        "\n",
        "    # Selects the top 75 streamers based on their prediction value (highest score).\n",
        "    predictionsBag = groupedStreamerWatchTime.topk(75 , key = lambda x : x[1])\n",
        "\n",
        "    predictions = predictionsBag.compute() # Computes and retrieves the top predictions.\n",
        "\n",
        "# map StreamerID back to Streamer name and print predictions in form of (Streamer Name , prediction value) sorted by prediction value\n",
        "    reverseDicitonary = dict() # Initializes an empty dictionary for reverse lookup (ID to Name).\n",
        "    for key , value in streamerIDlookUpDict.items(): # Populates the reverse dictionary.\n",
        "        reverseDicitonary[value] = key\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n     Predictions\\n     (Streamer Name , prediction ranking value) \\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "    for element in predictions: # Iterates through the computed predictions.\n",
        "\n",
        "        streamerID , value = element[0] , element[1] # Unpacks StreamerID and prediction value.\n",
        "        streamerName = reverseDicitonary[streamerID] # Gets the streamer name using the reverse lookup.\n",
        "        print(streamerName , value) # Prints the streamer name and its prediction value.\n",
        "\n",
        "# print the input data for the target user that the model \"saw\"\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n     Input Data (ie Data Seen By Model)\\n     (Streamer Name , total watch time for input user)\\n\" + \"=\"*40 + \"\\n\")\n",
        "    # Filters the omitted grouped data to get the specific target user's data.\n",
        "    targetUserData = foldedGroupedOmitted.filter(lambda x : x[0] == UserIDofInterest)\n",
        "    InputData = targetUserData.pluck(1).compute() # Extracts and computes the input data for the target user.\n",
        "    InputData = InputData[0] # Gets the first (and only) element.\n",
        "    InputData = sorted(InputData , key = lambda x : x[2]) # Sorts the input data by watch time.\n",
        "    InputData = InputData[::-1] # Reverses the sorted list to get descending order.\n",
        "\n",
        "    inputData2 = []\n",
        "\n",
        "    for element in InputData: # Iterates through the sorted input data.\n",
        "        element2 , element3 = element[1] , element[2] # Unpacks StreamerID and watch time.\n",
        "        inputData2.append(reverseDicitonary[element2]) # Appends the streamer name to a list of seen streamers.\n",
        "        print(reverseDicitonary[element2] , element3) # Prints the streamer name and watch time.\n",
        "\n",
        "\n",
        "# print the final streamer recomendations from the model\n",
        "# which are just all streamers that are in predictions (ie predicted by the model) but not in the input data\n",
        "# because we dont watch to recommend streamers that we know the target user already watches\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n     Recomendations\\n\" + \"=\"*40 + \"\\n\")\n",
        "    recomendations = []\n",
        "    for element in predictions: # Iterates through the generated predictions.\n",
        "        streamerID , value = element[0] , element[1] # Unpacks StreamerID and prediction value.\n",
        "        streamerName = reverseDicitonary[streamerID] # Gets the streamer name.\n",
        "        if streamerName not in inputData2: # Checks if the predicted streamer was NOT in the user's input data.\n",
        "            recomendations.append((streamerName , value)) # Adds it to recommendations if not seen before.\n",
        "\n",
        "    recomendations = recomendations[:k] # Truncates the recommendations to the top 'k' recommendations.\n",
        "\n",
        "    for streamerName , value in recomendations: # Prints the final recommendations.\n",
        "        print(streamerName , value)\n",
        "\n",
        "\n",
        "\n",
        "# print the ground truths that we ommitted from the data the model \"saw\"\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n     Omitted Data\\n\" + \"=\"*40 + \"\\n\")\n",
        "    TrainningDataSet = inputData2 # The streamers the model \"saw\" for the target user.\n",
        "    fullDataSet = [reverseDicitonary[element[1]] for element in fullDataSet] # Converts full dataset streamer IDs to names.\n",
        "\n",
        "    ommitedData = []\n",
        "    for element in fullDataSet: # Identifies streamers that were in the original full data but omitted for testing.\n",
        "        if element not in TrainningDataSet:\n",
        "            ommitedData.append(element)\n",
        "    print(ommitedData) # Prints the list of omitted streamers.\n",
        "\n",
        "# print the recovered ground truths , ie what streamers that are in ommited data and were also predicted by our model\n",
        "    print(\"\\n\" + \"=\"*20 + \"\\n     Correct Recomendations\\n\" + \"=\"*20 + \"\\n\")\n",
        "\n",
        "    correctRecomendations = []\n",
        "\n",
        "    for streamer , _ in recomendations: # Checks which of the recommendations are among the omitted data.\n",
        "        if streamer in ommitedData:\n",
        "            correctRecomendations.append(streamer)\n",
        "            print(streamer) # Prints correctly recommended (recovered) streamers.\n",
        "\n",
        "    print()\n",
        "    if len(ommitedData) != 0: # Calculates and prints Recall@K if there was omitted data.\n",
        "        print(\"Recall at K : \" , len(correctRecomendations) / len(ommitedData))\n",
        "    else:\n",
        "        print(\"Recall at K : 0\") # If no data was omitted, Recall@K is 0.\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    cluster = LocalCluster(n_workers= 1, threads_per_worker=1) # Initializes a local Dask cluster with 1 worker and 1 thread.\n",
        "    client = cluster.get_client() # Gets a Dask client connected to the local cluster.\n",
        "\n",
        "\n",
        "    run_model(1, 25000, 8 , 20) # Calls the run_model function with specified parameters:\n",
        "                                # UserIDofInterest = 1, problemSize = 25000, numberOfWorkers = 8, k = 20.\n",
        "                                # i choose these arguments for the test because user 1 has a good number of nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKpD11i76B9P"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}
